{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning a pre-trained Convolutional Network\n",
    "\n",
    "A practical approach to training a convolutional network is to fine-tune an existing pretrained convolutional network. This works well in cases where there is reasonable overlap between the classification tasks of the pre-trained network and the new classification task.\n",
    "\n",
    "We will show how to fine-tune an existing network trained on the imagenet classification task, to instead classify the \"styles\" of flickr images, from a Flickr style dataset. The dataset is already available on the Amazon EC2 instance, but if you're running this code from elsewhere, the dataset can easily be downloaded with scripts bundled with caffe:\n",
    "```\n",
    "scripts/download_model_binary.py models/bvlc_reference_caffenet\n",
    "python examples/finetune_flickr_style/assemble_data.py --workers=-1 --images=2000 --seed=1701 --label=5\n",
    "```\n",
    "\n",
    "The dataset consists of photos from flickr classified according to certain styles:\n",
    "![](./finetuning_example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "caffe_root = '/home/ubuntu/caffe/'\n",
    "import sys\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "import os\n",
    "os.chdir('../caffe')\n",
    "\n",
    "import caffe\n",
    "import numpy as np\n",
    "from pylab import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show what is the difference between the fine-tuning network and the original caffe model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1c1\r\n",
      "< name: \"CaffeNet\"\r\n",
      "---\r\n",
      "> name: \"FlickrStyleCaffeNet\"\r\n",
      "4c4\r\n",
      "<   type: \"Data\"\r\n",
      "---\r\n",
      ">   type: \"ImageData\"\r\n",
      "15,26c15,19\r\n",
      "< # mean pixel / channel-wise mean instead of mean image\r\n",
      "< #  transform_param {\r\n",
      "< #    crop_size: 227\r\n",
      "< #    mean_value: 104\r\n",
      "< #    mean_value: 117\r\n",
      "< #    mean_value: 123\r\n",
      "< #    mirror: true\r\n",
      "< #  }\r\n",
      "<   data_param {\r\n",
      "<     source: \"examples/imagenet/ilsvrc12_train_lmdb\"\r\n",
      "<     batch_size: 256\r\n",
      "<     backend: LMDB\r\n",
      "---\r\n",
      ">   image_data_param {\r\n",
      ">     source: \"data/flickr_style/train.txt\"\r\n",
      ">     batch_size: 50\r\n",
      ">     new_height: 256\r\n",
      ">     new_width: 256\r\n",
      "31c24\r\n",
      "<   type: \"Data\"\r\n",
      "---\r\n",
      ">   type: \"ImageData\"\r\n",
      "42,51c35,36\r\n",
      "< # mean pixel / channel-wise mean instead of mean image\r\n",
      "< #  transform_param {\r\n",
      "< #    crop_size: 227\r\n",
      "< #    mean_value: 104\r\n",
      "< #    mean_value: 117\r\n",
      "< #    mean_value: 123\r\n",
      "< #    mirror: true\r\n",
      "< #  }\r\n",
      "<   data_param {\r\n",
      "<     source: \"examples/imagenet/ilsvrc12_val_lmdb\"\r\n",
      "---\r\n",
      ">   image_data_param {\r\n",
      ">     source: \"data/flickr_style/test.txt\"\r\n",
      "53c38,39\r\n",
      "<     backend: LMDB\r\n",
      "---\r\n",
      ">     new_height: 256\r\n",
      ">     new_width: 256\r\n",
      "323a310\r\n",
      ">   # Note that lr_mult can be set to 0 to disable any fine-tuning of this, and any other, layer\r\n",
      "360c347\r\n",
      "<   name: \"fc8\"\r\n",
      "---\r\n",
      ">   name: \"fc8_flickr\"\r\n",
      "363c350,351\r\n",
      "<   top: \"fc8\"\r\n",
      "---\r\n",
      ">   top: \"fc8_flickr\"\r\n",
      ">   # lr_mult is set to higher than for other layers, because this layer is starting from random while the others are already trained\r\n",
      "365c353\r\n",
      "<     lr_mult: 1\r\n",
      "---\r\n",
      ">     lr_mult: 10\r\n",
      "369c357\r\n",
      "<     lr_mult: 2\r\n",
      "---\r\n",
      ">     lr_mult: 20\r\n",
      "373c361\r\n",
      "<     num_output: 1000\r\n",
      "---\r\n",
      ">     num_output: 20\r\n",
      "384a373,379\r\n",
      ">   name: \"loss\"\r\n",
      ">   type: \"SoftmaxWithLoss\"\r\n",
      ">   bottom: \"fc8_flickr\"\r\n",
      ">   bottom: \"label\"\r\n",
      ">   top: \"loss\"\r\n",
      "> }\r\n",
      "> layer {\r\n",
      "387c382\r\n",
      "<   bottom: \"fc8\"\r\n",
      "---\r\n",
      ">   bottom: \"fc8_flickr\"\r\n",
      "393,399d387\r\n",
      "< }\r\n",
      "< layer {\r\n",
      "<   name: \"loss\"\r\n",
      "<   type: \"SoftmaxWithLoss\"\r\n",
      "<   bottom: \"fc8\"\r\n",
      "<   bottom: \"label\"\r\n",
      "<   top: \"loss\"\r\n"
     ]
    }
   ],
   "source": [
    "!diff models/bvlc_reference_caffenet/train_val.prototxt models/finetune_flickr_style/train_val.prototxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your record, if you want to train the network in pure C++ tools, here is the command:\n",
    "\n",
    "<code>\n",
    "build/tools/caffe train \\\n",
    "    -solver models/finetune_flickr_style/solver.prototxt \\\n",
    "    -weights models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel \\\n",
    "    -gpu 0\n",
    "</code>\n",
    "\n",
    "However, we will train using Python in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, finetune_loss=3.360094, scratch_loss=3.136188\n",
      "iter 10, finetune_loss=2.672608, scratch_loss=9.736364\n",
      "iter 20, finetune_loss=2.071996, scratch_loss=2.250404\n",
      "iter 30, finetune_loss=1.758295, scratch_loss=2.049553\n",
      "iter 40, finetune_loss=1.533391, scratch_loss=1.941318\n",
      "iter 50, finetune_loss=1.561658, scratch_loss=1.839706\n",
      "iter 60, finetune_loss=1.461696, scratch_loss=1.880035\n",
      "iter 70, finetune_loss=1.267941, scratch_loss=1.719161\n",
      "iter 80, finetune_loss=1.192778, scratch_loss=1.627453\n",
      "iter 90, finetune_loss=1.541176, scratch_loss=1.822061\n",
      "iter 100, finetune_loss=1.029039, scratch_loss=1.654087\n",
      "iter 110, finetune_loss=1.138547, scratch_loss=1.735837\n",
      "iter 120, finetune_loss=0.917412, scratch_loss=1.851918\n",
      "iter 130, finetune_loss=0.971519, scratch_loss=1.801927\n",
      "iter 140, finetune_loss=0.868252, scratch_loss=1.745545\n",
      "iter 150, finetune_loss=0.790020, scratch_loss=1.844925\n",
      "iter 160, finetune_loss=1.092668, scratch_loss=1.695591\n",
      "iter 170, finetune_loss=1.055344, scratch_loss=1.661715\n",
      "iter 180, finetune_loss=0.969769, scratch_loss=1.823639\n",
      "iter 190, finetune_loss=0.780566, scratch_loss=1.820862\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "niter = 400\n",
    "test_interval = 25\n",
    "# losses will also be stored in the log\n",
    "train_loss = np.zeros(niter)\n",
    "test_acc = zeros(int(np.ceil(niter / test_interval)))\n",
    "scratch_train_loss = np.zeros(niter)\n",
    "\n",
    "caffe.set_device(0)\n",
    "caffe.set_mode_gpu()\n",
    "# We create a solver that fine-tunes from a previously trained network.\n",
    "solver = caffe.SGDSolver('models/finetune_flickr_style/solver.prototxt')\n",
    "solver.net.copy_from('models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel')\n",
    "# For reference, we also create a solver that does no finetuning.\n",
    "#scratch_solver = caffe.SGDSolver('models/finetune_flickr_style/solver.prototxt')\n",
    "\n",
    "# We run the solver for niter times, and record the training loss.\n",
    "for it in range(niter):\n",
    "    solver.step(1)  # SGD by Caffe\n",
    "    #scratch_solver.step(1)\n",
    "    # store the train loss\n",
    "    train_loss[it] = solver.net.blobs['loss'].data\n",
    "    #scratch_train_loss[it] = scratch_solver.net.blobs['loss'].data\n",
    "    if it % 10 == 0:\n",
    "        #print 'iter %d, finetune_loss=%f, scratch_loss=%f' % (it, train_loss[it], scratch_train_loss[it])\n",
    "        print 'iter %d, finetune_loss=%f' % (it, train_loss[it])\n",
    "    if it % test_interval == 0:\n",
    "        print 'Iteration', it, 'testing...'\n",
    "        correct = 0\n",
    "        for test_it in range(100):\n",
    "            solver.test_nets[0].forward()\n",
    "            correct += sum(solver.test_nets[0].blobs['fc8_flickr'].data.argmax(1)\n",
    "                           == solver.test_nets[0].blobs['label'].data)\n",
    "        test_acc[it // test_interval] = correct / 5000.\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot training loss and accuracy\n",
    "rcParams['figure.figsize'] = (20, 7)\n",
    "pl1 = subplot(1,2,1)\n",
    "pl1.plot(arange(niter), train_loss)\n",
    "pl1.set_ylabel('train_loss')\n",
    "pl2 = subplot(1,2,2)\n",
    "pl2.plot(test_interval * arange(len(test_acc)), test_acc*0.1, 'r')\n",
    "pl2.set_ylabel('test_accuracy')\n",
    "pl2.set_xlabel('iteration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huzzah! So we did finetuning and it is awesome. Let's take a look at what kind of results we are able to get with a longer, more complete run of the style recognition dataset. Note: the below URL might be occassionally down because it is run on a research machine.\n",
    "\n",
    "http://demo.vislab.berkeleyvision.org/"
   ]
  }
 ],
 "metadata": {
  "description": "Fine-tune the ImageNet-trained CaffeNet on new data.",
  "example_name": "Fine-tuning for Style Recognition",
  "include_in_docs": true,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "priority": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
